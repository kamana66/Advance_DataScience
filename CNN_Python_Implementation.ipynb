{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN(Convolutional Neural Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A CNN is a type of neural network that automatically learns patterns and features (edges, textures, shapes) from raw data using convolution operations, rather than relying on manual feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core components of a CNN\n",
    "1. Convolutional layer\n",
    "\n",
    "Applies filters (kernels) across the input\n",
    "\n",
    "Detects features like edges, corners, textures\n",
    "\n",
    "Produces feature maps\n",
    "\n",
    "2. Activation function\n",
    "\n",
    "Usually ReLU\n",
    "\n",
    "Adds non-linearity so the network can learn complex patterns\n",
    "\n",
    "3. Pooling layer\n",
    "\n",
    "Reduces spatial size (e.g., max pooling)\n",
    "\n",
    "Makes the model more efficient and robust to small shifts\n",
    "\n",
    "4. Fully connected layer\n",
    "\n",
    "Combines extracted features\n",
    "\n",
    "Outputs predictions (class labels, probabilities, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why CNNs are important in data science\n",
    "\n",
    "CNNs excel at:\n",
    "\n",
    "- Image classification (cats vs dogs, medical scans)\n",
    "\n",
    "- Object detection (faces, cars, people)\n",
    "\n",
    "- OCR & document analysis\n",
    "\n",
    "- Video analysis\n",
    "\n",
    "- Spatial data and some time-series problems\n",
    "\n",
    "They drastically reduce the need for handcrafted features, which was a major limitation in classical machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0leuqjoYujON"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rOOh2kyaCjPf"
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv('/content/car data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9lhvdT8VvRuM"
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S__uBGRiw8NC"
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv('/content/drive/MyDrive/Colab folder/car data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QtRW8zl4FE6d"
   },
   "source": [
    "# Why convolution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "yqyi-U3NWatQ"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_NSsRJOFMN7"
   },
   "source": [
    "SPATIAL INVARIANCE or LOSS IN FEATURES\n",
    "\n",
    "The spatial features of a 2D image are lost when it is flattened to a 1D vector input. Before feeding an image to the hidden layers of an MLP, we must flatten the image matrix to a 1D vector. This implies that all of the image's 2D information is discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b8yceI2SFLa3"
   },
   "outputs": [],
   "source": [
    "from google.colab.patches import cv2_imshow\n",
    "import os, json, cv2, random\n",
    "\n",
    "!wget http://images.cocodataset.org/val2017/000000439715.jpg -q -O input.jpg\n",
    "im = cv2.imread(\"./input.jpg\")\n",
    "cv2_imshow(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m6A0cIPvFLdk"
   },
   "outputs": [],
   "source": [
    "im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yTNZ5o59MK-0"
   },
   "source": [
    "#### Sample Image\n",
    "\n",
    "![title](https://aishack.in/static/img/tut/conv-gaussian-blur.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mW3v8Q-aMQWQ"
   },
   "source": [
    "## Increase in  Parameter Issue\n",
    "\n",
    "While increase in  Parameter Issue is not a big problem for the\n",
    "MNIST dataset because the images are really small in size (28 × 28), what happens when we try to process larger images?\n",
    "\n",
    "For example, if we have an image with dimensions 1,000 × 1,000, it will yield 1 million parameters for each node in the first hidden layer.\n",
    "\n",
    "- So if the first hidden layer has 1,000 neurons, this will yield 1 billion parameters even in such a small network. You can imagine the computational complexity of optimizing 1 billion parameters after only the first layer.\n",
    "\n",
    "\n",
    "### Fully Connected Neural Net\n",
    "\n",
    "![title](https://www.researchgate.net/profile/Arvind-Sreenivas/publication/343263135/figure/fig3/AS:918277995905024@1595945943003/Fully-connected-layer.jpg)\n",
    "\n",
    "\n",
    "### Local Connected Neural Net\n",
    "\n",
    "![title](https://www.cs.toronto.edu/~lczhang/360/lec/w04/imgs/local.png)\n",
    "\n",
    "[Source](https://www.cs.toronto.edu)\n",
    "\n",
    "\n",
    "### Guide for design of a neural network architecture suitable for computer vision\n",
    "\n",
    "- In the earliest layers, our network should respond similarly to the same patch, regardless of where it appears in the image. This principle is called translation invariance.\n",
    "- The earliest layers of the network should focus on local regions, without regard for the contents of the image in distant regions. This is the locality principle. Eventually, these local representations can be aggregated to make predictions at the whole image level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54MAtx30UmG1"
   },
   "source": [
    "### Human Brain Visual Cortex processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJz3CL5_UpiE"
   },
   "source": [
    "![](https://www.researchgate.net/profile/Bruno-Cessac/publication/233971662/figure/fig1/AS:393541936271366@1470839117205/Processing-steps-of-the-visual-stream-a-The-cellular-organization-of-the-retina-from.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fJQm3N63-izT"
   },
   "outputs": [],
   "source": [
    "https://distill.pub/2017/feature-visualization/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5Co8rmVPGgN"
   },
   "source": [
    "# What are Convolutional Neural Networks?\n",
    "\n",
    "\n",
    "Convolutional Neural Networks (ConvNets or CNNs) are a category of Neural Networks that have proven very effective in areas such as image recognition and classification. ConvNets have been successful in identifying faces, objects and traffic signs apart from powering vision in robots and self driving cars.\n",
    "\n",
    "\n",
    "A Convolutional Neural Network (CNN) is comprised of one or more convolutional layers (often with a subsampling step) and then followed by one or more fully connected layers as in a standard multilayer neural network. The architecture of a CNN is designed to take advantage of the 2D structure of an input image (or other 2D input such as a speech signal). This is achieved with local connections and tied weights followed by some form of pooling which results in translation invariant features. Another benefit of CNNs is that they are easier to train and have many fewer parameters than fully connected networks with the same number of hidden units. In this article we will discuss the architecture of a CNN and the back propagation algorithm to compute the gradient with respect to the parameters of the model in order to use gradient based optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vT4qMk6cQRbK"
   },
   "source": [
    "## Visualizing the Process\n",
    "\n",
    "\n",
    "## Simple Convolution\n",
    "\n",
    "![](https://miro.medium.com/max/1400/1*Fw-ehcNBR9byHtho-Rxbtw.gif)\n",
    "\n",
    "## Matrix Calculation\n",
    "\n",
    "![](https://miro.medium.com/max/535/1*Zx-ZMLKab7VOCQTxdZ1OAw.gif)\n",
    "\n",
    "## Padding Concept\n",
    "![](https://miro.medium.com/max/395/1*1okwhewf5KCtIPaFib4XaA.gif)\n",
    "\n",
    "## Stride Concept\n",
    "![](https://miro.medium.com/max/294/1*BMngs93_rm2_BpJFH2mS0Q.gif)\n",
    "\n",
    "## Feature Accumulation\n",
    "![](https://miro.medium.com/max/2000/1*8dx6nxpUh2JqvYWPadTwMQ.gif)\n",
    "\n",
    "## Feature Aggregation\n",
    "![](https://miro.medium.com/max/2000/1*CYB2dyR3EhFs1xNLK8ewiA.gif)\n",
    "\n",
    "## Convolution Operation\n",
    "\n",
    "![](https://cdn-media-1.freecodecamp.org/images/gb08-2i83P5wPzs3SL-vosNb6Iur5kb5ZH43)\n",
    "\n",
    "\n",
    "[Source](https://https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1)\n",
    "\n",
    "[Source](https://cs231n.github.io/convolutional-networks/)\n",
    "\n",
    "\n",
    "# The CNN Complete Network Overview\n",
    "\n",
    "![CNN Image](https://res.cloudinary.com/practicaldev/image/fetch/s--w1RZuJPn--/c_imagga_scale,f_auto,fl_progressive,h_420,q_auto,w_1000/https://dev-to-uploads.s3.amazonaws.com/i/1inc9c00m35q12lidqde.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9RwK_ZxNACXg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-AXRLiTOoOoc"
   },
   "source": [
    "# Best Place to Explore Kernels\n",
    "\n",
    "[Kernels](https://setosa.io/ev/image-kernels/)\n",
    "\n",
    "[Kernels as Edge Detector](https://aishack.in/tutorials/image-convolution-examples/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kEousPCUpZVL"
   },
   "source": [
    "### Features extracted by Kernels\n",
    "\n",
    "![](https://cs231n.github.io/assets/cnn/weights.jpeg)\n",
    "\n",
    "### Features > Patterns > Parts of Object\n",
    "![](http://media5.datahacker.rs/2018/10/features_3images.png)\n",
    "\n",
    "[Source](https://cs231n.github.io/convolutional-networks/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ipv2dCj81QJq"
   },
   "source": [
    "## Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQGlr4OD1Vv7"
   },
   "source": [
    "Let's develop better intuition for how Convolutional Neural Networks (CNN) work. We'll examine how humans classify images, and then see how CNNs use similar approaches.\n",
    "\n",
    "Let’s say we wanted to classify the following image of a dog as a Golden Retriever:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J-LsWdna1arI"
   },
   "source": [
    "<img src=\"https://d17h27t6h515a5.cloudfront.net/topher/2016/November/58377b77_dog-1210559-1280/dog-1210559-1280.jpg\" width=\"500\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6DJy1MK01cSw"
   },
   "source": [
    "As humans, how do we do this?\n",
    "\n",
    "One thing we do is that we identify certain parts of the dog, such as the nose, the eyes, and the fur. We essentially break up the image into smaller pieces, recognize the smaller pieces, and then combine those pieces to get an idea of the overall dog.\n",
    "\n",
    "In this case, we might break down the image into a combination of the following:\n",
    "\n",
    "* A nose\n",
    "* Two eyes\n",
    "* Golden fur\n",
    "\n",
    "These pieces can be seen below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iUfN-DM61fW8"
   },
   "source": [
    "As humans, how do we do this?\n",
    "\n",
    "One thing we do is that we identify certain parts of the dog, such as the nose, the eyes, and the fur. We essentially break up the image into smaller pieces, recognize the smaller pieces, and then combine those pieces to get an idea of the overall dog.\n",
    "\n",
    "In this case, we might break down the image into a combination of the following:\n",
    "\n",
    "* A nose\n",
    "* Two eyes\n",
    "* Golden fur\n",
    "\n",
    "These pieces can be seen below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iU-mLyj21i7x"
   },
   "source": [
    "\n",
    "<img src=\"https://d17h27t6h515a5.cloudfront.net/topher/2016/November/58377bdb_screen-shot-2016-11-24-at-12.49.08-pm/screen-shot-2016-11-24-at-12.49.08-pm.png\" width=\"250\" height=\"250\">\n",
    "<center>The eye of the dog.</center>\n",
    "<br>\n",
    "\n",
    "<img src=\"https://d17h27t6h515a5.cloudfront.net/topher/2016/November/58377bed_screen-shot-2016-11-24-at-12.49.43-pm/screen-shot-2016-11-24-at-12.49.43-pm.png\" width=\"250\" height=\"250\">\n",
    "<center>The nose of the dog.</center>\n",
    "<br>\n",
    "\n",
    "<img src=\"https://d17h27t6h515a5.cloudfront.net/topher/2016/November/58377bff_screen-shot-2016-11-24-at-12.50.54-pm/screen-shot-2016-11-24-at-12.50.54-pm.png\" width=\"250\" height=\"250\">\n",
    "<center>The fur of the dog.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36ubS2WE3HYn"
   },
   "source": [
    "### Going One Step Further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qZHwtCQX3Nti"
   },
   "source": [
    "But let’s take this one step further. How do we determine what exactly a nose is? A Golden Retriever nose can be seen as an oval with two black holes inside it. Thus, one way of classifying a Retriever’s nose is to to break it up into smaller pieces and look for black holes (nostrils) and curves that define an oval as shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zr4q2gQD3UxD"
   },
   "source": [
    "<img src=\"https://d17h27t6h515a5.cloudfront.net/topher/2016/November/58377c52_screen-shot-2016-11-24-at-12.51.47-pm/screen-shot-2016-11-24-at-12.51.47-pm.png\">\n",
    "<center>A curve that we can use to determine a nose</center>\n",
    "<br>\n",
    "\n",
    "<img src=\"https://d17h27t6h515a5.cloudfront.net/topher/2016/November/58377c68_screen-shot-2016-11-24-at-12.51.51-pm/screen-shot-2016-11-24-at-12.51.51-pm.png\">\n",
    "<center>A nostril that we can use to classify the nose of the dog</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HEvB_--G3YrC"
   },
   "source": [
    "Broadly speaking, this is what a CNN learns to do. It learns to recognize basic lines and curves, then shapes and blobs, and then increasingly complex objects within the image. Finally, the CNN classifies the image by combining the larger, more complex objects.\n",
    "\n",
    "In our case, the levels in the hierarchy are:\n",
    "\n",
    "* Simple shapes, like ovals and dark circles\n",
    "* Complex objects (combinations of simple shapes), like eyes, nose, and fur\n",
    "* The dog as a whole (a combination of complex objects)\n",
    "\n",
    "With deep learning, we don't actually program the CNN to recognize these specific features. Rather, the CNN learns on its own to recognize such objects through forward propagation and backpropagation!\n",
    "\n",
    "It's amazing how well a CNN can learn to classify images, even though we never program the CNN with information about specific features to look for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-HHNNyzX3ZmE"
   },
   "source": [
    "![](https://d17h27t6h515a5.cloudfront.net/topher/2016/November/583cb19d_heirarchy-diagram/heirarchy-diagram.jpg)\n",
    "<center>An example of what each layer in a CNN might recognize when classifying a picture of a dog</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oShz1q6n3dVs"
   },
   "source": [
    "A CNN might have several layers, and each layer might capture a different level in the hierarchy of objects. The first layer is the lowest level in the hierarchy, where the CNN generally classifies small parts of the image into simple shapes like horizontal and vertical lines and simple blobs of colors. The subsequent layers tend to be higher levels in the hierarchy and generally classify more complex ideas like shapes (combinations of lines), and eventually full objects like dogs.\n",
    "\n",
    "Once again, the CNN **learns all of this on its own**. We don't ever have to tell the CNN to go looking for lines or curves or noses or fur. The CNN just learns from the training set and discovers which characteristics of a Golden Retriever are worth looking for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tL7w7sEB3gzy"
   },
   "source": [
    "A CNN might have several layers, and each layer might capture a different level in the hierarchy of objects. The first layer is the lowest level in the hierarchy, where the CNN generally classifies small parts of the image into simple shapes like horizontal and vertical lines and simple blobs of colors. The subsequent layers tend to be higher levels in the hierarchy and generally classify more complex ideas like shapes (combinations of lines), and eventually full objects like dogs.\n",
    "\n",
    "Once again, the CNN **learns all of this on its own**. We don't ever have to tell the CNN to go looking for lines or curves or noses or fur. The CNN just learns from the training set and discovers which characteristics of a Golden Retriever are worth looking for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iWXKErQRAp0I"
   },
   "source": [
    "# Python Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03zuYPtkyB66"
   },
   "source": [
    "To classify given image in to 10 classes of\n",
    "'airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nO2U9t3frojQ"
   },
   "outputs": [],
   "source": [
    "['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "atVFp1zEEBwB"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "# load the pre-shuffled train and test data\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()#50000 data in training and 10000 data in testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xvdszZBKXRh7"
   },
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hUOte0PwELKM"
   },
   "source": [
    "### 2. Visualize the First 24 Training Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zg_6ohoIEKXs"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "for i in range(36):\n",
    "    ax = fig.add_subplot(3, 12, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(x_train[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lAVERQHGESD1"
   },
   "source": [
    "### 3. Rescale the Images by Dividing Every Pixel in Every Image by 255\n",
    "In fact, the cost function has the shape of a bowl, but it can be an elongated bowl if the features have very\n",
    "different scales. Figure below shows Gradient Descent on a training set where features 1 and 2 have the\n",
    "same scale (on the left), and on a training set where feature 1 has much smaller values than feature 2 (on\n",
    "the right).\n",
    "\n",
    "** Tip: ** When using Gradient Descent, you should ensure that all features have a similar scale to speed up training or else it will take much longer to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yhsD2u-cdKs4"
   },
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4zk_on1yETMu"
   },
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')/255\n",
    "x_test = x_test.astype('float32')/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hSUzwVTNEcNY"
   },
   "source": [
    "### 4.  Break Dataset into Training, Testing, and Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hH-aYC9Adqfh"
   },
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J-82TmJ2Ec4q"
   },
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "#from tensorflow import keras\n",
    "\n",
    "# one-hot encode the labels\n",
    "num_classes = len(np.unique(y_train))\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)#Conver the array value(Class vector) into matrix\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# break training set into training and validation sets\n",
    "(x_train, x_valid) = x_train[5000:], x_train[:5000]\n",
    "(y_train, y_valid) = y_train[5000:], y_train[:5000]\n",
    "\n",
    "# print shape of training set\n",
    "print('x_train shape:', x_train.shape) # Output like How many number of images, image size, image size, How mant channels(RGB)\n",
    "\n",
    "# print number of training, validation, and test images\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "print(x_valid.shape[0], 'validation samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5-ne4T_9ElSl"
   },
   "source": [
    "### 5. Define the Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-v2-LKMVEZCe"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=16, kernel_size=3, padding='same', activation='relu',input_shape=(32, 32, 3)))\n",
    "#16 filters with 2X2 kernel with padding size same as iput and output\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.3)) # 30% nurons are deactivated to avoid overfitting\n",
    "model.add(Flatten())# Coverting data into 1D array\n",
    "model.add(Dense(500, activation='relu'))#500 nurons with relu AF\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ymR5BLFhEslL"
   },
   "source": [
    "### 6. Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lA4fBPwsEtcO"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V8x2e5BkEx2S"
   },
   "source": [
    "### 7. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q7UTvA6EE0lY"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint #callbacks are used to save the best weights for our model\n",
    "\n",
    "# train the model\n",
    "checkpointer = ModelCheckpoint(filepath='model.weights.best.hdf5', verbose=1, save_best_only=True)\n",
    "\n",
    "hist = model.fit(x_train, y_train, batch_size=32, epochs=2,\n",
    "          validation_data=(x_valid, y_valid), callbacks=[checkpointer],\n",
    "          verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TxsVftZZE6rA"
   },
   "source": [
    "### 8. Load the Model with the Best Validation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "76Be_1h4E3uI"
   },
   "outputs": [],
   "source": [
    "model.load_weights('model.weights.best.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AT6VNFoDFC61"
   },
   "source": [
    "### 9. Visualize Some Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bUvWMYd9FA0T"
   },
   "outputs": [],
   "source": [
    "# get predictions on the test set\n",
    "y_hat = model.predict(x_test)\n",
    "\n",
    "# define text labels (source: https://www.cs.toronto.edu/~kriz/cifar.html)\n",
    "cifar10_labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xNL1rrgZFPds"
   },
   "outputs": [],
   "source": [
    "# plot a random sample of test images, their predicted labels, and ground truth\n",
    "fig = plt.figure(figsize=(20, 8))\n",
    "for i, idx in enumerate(np.random.choice(x_test.shape[0], size=32, replace=False)):\n",
    "    ax = fig.add_subplot(4, 8, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(x_test[idx]))\n",
    "    pred_idx = np.argmax(y_hat[idx])\n",
    "    true_idx = np.argmax(y_test[idx])\n",
    "    ax.set_title(\"{} ({})\".format(cifar10_labels[pred_idx], cifar10_labels[true_idx]),\n",
    "                 color=(\"blue\" if pred_idx == true_idx else \"red\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PggMH3H2CWPe"
   },
   "outputs": [],
   "source": [
    "# evaluate test accuracy\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "accuracy = 100*score[1]\n",
    "\n",
    "# print test accuracy\n",
    "print('Test accuracy: %.4f%%' % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kClksF655lSc"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fD4lmyAvCsUL"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing import image\n",
    "img = tf.keras.utils.load_img(\"/content/drive/MyDrive/My Data/06-Advance Data Science/AIF-1-DL-Foundation-Course-on-CNN/Capture.JPG\", target_size=(32,32,3))\n",
    "img_array = keras.preprocessing.image.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, 0)  # Create batch axis\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = predictions[0]\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bSCYu2Qmivbr"
   },
   "outputs": [],
   "source": [
    "['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VB2GLlLJF5G-"
   },
   "outputs": [],
   "source": [
    "cifar10_labels[np.argmax(score)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C_GSxgXm0uMQ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
